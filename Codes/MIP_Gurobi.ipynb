{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628feb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gurobipy import Model, GRB, quicksum\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d13bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def build_and_solve_xor(hidden_units=3,\n",
    "                        W_bound=5.0, V_bound=5.0, B_bound=5.0, C_bound=5.0,\n",
    "                        pwl_range=8.0, pwl_points=21,\n",
    "                        time_limit=30, verbose=True, seed=0):\n",
    "    \"\"\"\n",
    "    MILP for XOR with one hidden ReLU layer and sigmoid output (PWL).\n",
    "    Loss = L1(y - yhat). Hidden_units in {2,3,4}.\n",
    "    \"\"\"\n",
    "\n",
    "    assert hidden_units in [2, 3, 4], \"hidden_units must be 2, 3, or 4\"\n",
    "\n",
    "    # XOR dataset\n",
    "    X = np.array([[0.,0.],\n",
    "                  [0.,1.],\n",
    "                  [1.,0.],\n",
    "                  [1.,1.]], dtype=float)\n",
    "    y = np.array([0., 1., 1., 0.], dtype=float)\n",
    "    N, d = X.shape\n",
    "    mH = hidden_units\n",
    "\n",
    "    # Big-M for pre-activation splitting (ReLU)\n",
    "    max_sum_abs_x = float(np.max(np.sum(np.abs(X), axis=1)))  # here = 2\n",
    "    M_act = W_bound * max_sum_abs_x + B_bound  # bounds |sum_j w_ij x_j + b_i|\n",
    "    # Bounds for products phi_{n,i} = p_{n,i} * v_i (need bounds on p and v)\n",
    "    pL, pU = 0.0, M_act\n",
    "    vL, vU = -V_bound, V_bound\n",
    "\n",
    "    # Bound for u_n = sum_i v_i * p_{n,i} + c\n",
    "    U_max = mH * V_bound * M_act + C_bound\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"M_act={M_act:.4g}, U_max={U_max:.4g}\")\n",
    "\n",
    "    # PWL grid for sigmoid on [-pwl_range, pwl_range]\n",
    "    xs = np.linspace(-pwl_range, pwl_range, pwl_points)\n",
    "    ys = sigmoid(xs)\n",
    "\n",
    "    # Start model\n",
    "    mdl = Model(\"xor_relu_sigmoid_milp\")\n",
    "    mdl.Params.Seed = seed\n",
    "    mdl.Params.OutputFlag = 1 if verbose else 0\n",
    "    if time_limit is not None:\n",
    "        mdl.Params.TimeLimit = time_limit\n",
    "\n",
    "    # Variables\n",
    "    w = mdl.addVars(mH, d, lb=-W_bound, ub=W_bound, vtype=GRB.CONTINUOUS, name=\"w\")\n",
    "    b = mdl.addVars(mH, lb=-B_bound, ub=B_bound, vtype=GRB.CONTINUOUS, name=\"b\")\n",
    "    v = mdl.addVars(mH, lb=-V_bound, ub=V_bound, vtype=GRB.CONTINUOUS, name=\"v\")\n",
    "    c = mdl.addVar(lb=-C_bound, ub=C_bound, vtype=GRB.CONTINUOUS, name=\"c\")\n",
    "\n",
    "    # ReLU split variables per sample & neuron\n",
    "    p = mdl.addVars(N, mH, lb=0.0, vtype=GRB.CONTINUOUS, name=\"p\")  # positive part (ReLU output)\n",
    "    q = mdl.addVars(N, mH, lb=0.0, vtype=GRB.CONTINUOUS, name=\"q\")  # negative part\n",
    "    z = mdl.addVars(N, mH, vtype=GRB.BINARY, name=\"z\")              # gate\n",
    "\n",
    "    # Bilinear linearization for output: phi_{n,i} = p_{n,i} * v_i\n",
    "    phi = mdl.addVars(N, mH, lb=-GRB.INFINITY, ub=GRB.INFINITY, vtype=GRB.CONTINUOUS, name=\"phi\")\n",
    "\n",
    "    # Output pre-sigmoid and post-sigmoid\n",
    "    u = mdl.addVars(N, lb=-U_max, ub=U_max, vtype=GRB.CONTINUOUS, name=\"u\")     # pre-sigmoid\n",
    "    yhat = mdl.addVars(N, lb=0.0, ub=1.0, vtype=GRB.CONTINUOUS, name=\"yhat\")    # approx sigmoid(u)\n",
    "\n",
    "    # L1 errors\n",
    "    epos = mdl.addVars(N, lb=0.0, vtype=GRB.CONTINUOUS, name=\"eplus\")\n",
    "    eneg = mdl.addVars(N, lb=0.0, vtype=GRB.CONTINUOUS, name=\"eminus\")\n",
    "\n",
    "    # Helper: add McCormick envelope for z = x*y, with bounds [xL,xU], [yL,yU]\n",
    "    def add_mccormick(zvar, xvar, yvar, xL, xU, yL, yU, tag):\n",
    "        mdl.addConstr(zvar >= xL * yvar + yL * xvar - xL * yL, name=f\"{tag}_lb1\")\n",
    "        mdl.addConstr(zvar >= xU * yvar + yU * xvar - xU * yU, name=f\"{tag}_lb2\")\n",
    "        mdl.addConstr(zvar <= xU * yvar + yL * xvar - xU * yL, name=f\"{tag}_ub1\")\n",
    "        mdl.addConstr(zvar <= xL * yvar + yU * xvar - xL * yU, name=f\"{tag}_ub2\")\n",
    "\n",
    "    # Constraints\n",
    "\n",
    "    # (A) Pre-activation split & ReLU gating\n",
    "    # sum_j w_ij x^n_j + b_i = p_{n,i} - q_{n,i}\n",
    "    for n in range(N):\n",
    "        for i in range(mH):\n",
    "            mdl.addConstr(\n",
    "                quicksum(w[i, j] * X[n, j] for j in range(d)) + b[i] == p[n, i] - q[n, i],\n",
    "                name=f\"preact[n{n},i{i}]\"\n",
    "            )\n",
    "            # gating:\n",
    "            mdl.addConstr(p[n, i] <= M_act * (1 - z[n, i]), name=f\"p_gate[n{n},i{i}]\")\n",
    "            mdl.addConstr(q[n, i] <= M_act * z[n, i],       name=f\"q_gate[n{n},i{i}]\")\n",
    "\n",
    "    # (B) Output linearization: phi_{n,i} = p_{n,i} * v_i (McCormick)\n",
    "    for n in range(N):\n",
    "        for i in range(mH):\n",
    "            add_mccormick(phi[n, i], p[n, i], v[i], pL, pU, vL, vU, tag=f\"phi[n{n},i{i}]\")\n",
    "\n",
    "    # (C) Pre-sigmoid output: u_n = sum_i phi_{n,i} + c\n",
    "    for n in range(N):\n",
    "        mdl.addConstr(u[n] == quicksum(phi[n, i] for i in range(mH)) + c, name=f\"u_def[n{n}]\")\n",
    "\n",
    "    # (D) Sigmoid via PWL: yhat_n ≈ sigmoid(u_n) on grid xs,ys\n",
    "    for n in range(N):\n",
    "        mdl.addGenConstrPWL(u[n], yhat[n], xs.tolist(), ys.tolist(), name=f\"sig_pwl[n{n}]\")\n",
    "\n",
    "    # (E) L1 loss: y_n - yhat_n = epos_n - eneg_n\n",
    "    for n in range(N):\n",
    "        mdl.addConstr(y[n] - yhat[n] == epos[n] - eneg[n], name=f\"abs_err[n{n}]\")\n",
    "\n",
    "    # Objective: minimize sum of absolute errors\n",
    "    mdl.setObjective(quicksum(epos[n] + eneg[n] for n in range(N)), GRB.MINIMIZE)\n",
    "\n",
    "    # Solve\n",
    "    mdl.optimize()\n",
    "\n",
    "    # Extract solution\n",
    "    status = mdl.Status\n",
    "    if status not in (GRB.OPTIMAL, GRB.TIME_LIMIT):\n",
    "        raise RuntimeError(f\"Gurobi ended with status {status}\")\n",
    "\n",
    "    W = np.array([[w[i, j].X for j in range(d)] for i in range(mH)])\n",
    "    b_vec = np.array([b[i].X for i in range(mH)])\n",
    "    v_vec = np.array([v[i].X for i in range(mH)])\n",
    "    c_val = c.X\n",
    "    yhat_val = np.array([yhat[n].X for n in range(N)])\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nLearned parameters:\")\n",
    "        print(\"W =\\n\", W)\n",
    "        print(\"b =\", b_vec)\n",
    "        print(\"v =\", v_vec)\n",
    "        print(\"c =\", c_val)\n",
    "        print(\"Predictions yhat =\", yhat_val.round(4))\n",
    "        print(\"Targets     y    =\", y)\n",
    "\n",
    "    return {\n",
    "        \"W\": W, \"b\": b_vec, \"v\": v_vec, \"c\": c_val,\n",
    "        \"yhat\": yhat_val, \"model\": mdl\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Try with 2–4 hidden neurons\n",
    "    for mH in [2, 3, 4]:\n",
    "        print(f\"\\n=== Training XOR with {mH} hidden ReLU neurons and sigmoid output ===\")\n",
    "        res = build_and_solve_xor(hidden_units=mH,\n",
    "                                  W_bound=5.0, V_bound=5.0, B_bound=5.0, C_bound=5.0,\n",
    "                                  pwl_range=8.0, pwl_points=21,\n",
    "                                  time_limit=30, verbose=True, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d23804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_act=4.78973, M_sparsity=1\n",
      "Set parameter Seed to value 42\n",
      "Set parameter TimeLimit to value 30\n",
      "Set parameter OutputFlag to value 1\n",
      "Gurobi Optimizer version 12.0.2 build v12.0.2rc0 (mac64[arm] - Darwin 24.5.0 24F74)\n",
      "\n",
      "CPU model: Apple M3\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Non-default parameters:\n",
      "TimeLimit  30\n",
      "Seed  42\n",
      "\n",
      "Optimize a model with 246 rows, 140 columns and 696 nonzeros\n",
      "Model fingerprint: 0xa375b166\n",
      "Variable types: 104 continuous, 36 integer (36 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [4e-02, 5e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [2e-02, 5e+00]\n",
      "Found heuristic solution: objective 3.0000000\n",
      "Presolve time: 0.00s\n",
      "Presolved: 246 rows, 140 columns, 696 nonzeros\n",
      "Variable types: 104 continuous, 36 integer (36 binary)\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 102 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00000    0    7    3.00000    0.00000   100%     -    0s\n",
      "H    0     0                       0.0000000    0.00000  0.00%     -    0s\n",
      "     0     0    0.00000    0    7    0.00000    0.00000  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (102 simplex iterations) in 0.01 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 2: 0 3 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 0.000000000000e+00, best bound 0.000000000000e+00, gap 0.0000%\n",
      "\n",
      "Learned parameters:\n",
      "W =\n",
      " [[-0.52363467 -0.52363467  0.26649869]\n",
      " [-0.49192842 -1.         -1.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.22934502  0.52925597 -0.02888002]]\n",
      "b = [-1.         -1.          0.          0.44685943]\n",
      "v = [-0.47636533  0.         -1.         -0.47074403]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def build_and_solve_nn_milp(X, y, l1,\n",
    "                            W_bound=1.0, V_bound=1.0, B_bound=1.0,\n",
    "                            time_limit=None, verbose=True, seed=0):\n",
    "    \"\"\"\n",
    "    McCormick linearizations\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array (N, d)\n",
    "        Input data.\n",
    "    y : array (N,)\n",
    "        Targets.\n",
    "    l1 : int\n",
    "        Number of hidden units.\n",
    "    W_bound, V_bound, B_bound : float\n",
    "        Symmetric bounds: w_ij ∈ [-W_bound, W_bound], v_i ∈ [-V_bound, V_bound], b_i ∈ [-B_bound, B_bound].\n",
    "        These bounds are required for McCormick linearizations and big-M safety.\n",
    "    time_limit : float or None\n",
    "        Optional solver time limit in seconds.\n",
    "    verbose : bool\n",
    "        Print logs and learned parameters.\n",
    "    seed : int\n",
    "        Random seed for solver.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with learned parameters: W (l1,d), b (l1,), v (l1,)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    N, d = X.shape\n",
    "\n",
    "    # === Big-Ms ===\n",
    "    # For activation p,q gating, need M_act >= max |sum_j w_ij x^n_j + b_i|\n",
    "    # Use conservative bound via norms:\n",
    "    max_sum_abs_x = float(np.max(np.sum(np.abs(X), axis=1))) if N > 0 else 0.0\n",
    "    M_act = W_bound * max_sum_abs_x + B_bound\n",
    "    # For s_{ij} = w_{ij} v_i, use M_sparsity >= max |w * v|\n",
    "    M_sparsity = W_bound * V_bound\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"M_act={M_act:.6g}, M_sparsity={M_sparsity:.6g}\")\n",
    "\n",
    "    m = Model(\"nn_milp\")\n",
    "    m.Params.Seed = seed\n",
    "    if time_limit is not None:\n",
    "        m.Params.TimeLimit = time_limit\n",
    "    m.Params.OutputFlag = 1 if verbose else 0\n",
    "\n",
    "    # === Variables ===\n",
    "    # Weights and bias\n",
    "    w = m.addVars(l1, d, lb=-W_bound, ub=W_bound, vtype=GRB.CONTINUOUS, name=\"w\")\n",
    "    b = m.addVars(l1, lb=-B_bound, ub=B_bound, vtype=GRB.CONTINUOUS, name=\"b\")\n",
    "    v = m.addVars(l1, lb=-V_bound, ub=V_bound, vtype=GRB.CONTINUOUS, name=\"v\")\n",
    "\n",
    "    # Activations split variables p, q and gating z\n",
    "    p = m.addVars(N, l1, lb=0.0, vtype=GRB.CONTINUOUS, name=\"p\")\n",
    "    q = m.addVars(N, l1, lb=0.0, vtype=GRB.CONTINUOUS, name=\"q\")\n",
    "    z = m.addVars(N, l1, vtype=GRB.BINARY, name=\"z\")\n",
    "\n",
    "    # s_{ij} ~ w_{ij} * v_i  (McCormick) and sparsity switch t_{ij}\n",
    "    s = m.addVars(l1, d, lb=-GRB.INFINITY, ub=GRB.INFINITY, vtype=GRB.CONTINUOUS, name=\"s\")\n",
    "    t = m.addVars(l1, d, vtype=GRB.BINARY, name=\"t\")\n",
    "\n",
    "    # phi_{n,i} ~ p_{n,i} * v_i  (for output sum_i p_i^n v_i = y^n)\n",
    "    phi = m.addVars(N, l1, lb=-GRB.INFINITY, ub=GRB.INFINITY, vtype=GRB.CONTINUOUS, name=\"phi\")\n",
    "\n",
    "    # === Helper: add McCormick envelope for z = x * y, with bounds [xL,xU], [yL,yU] ===\n",
    "    def add_mccormick(zvar, xvar, yvar, xL, xU, yL, yU, tag=\"mc\"):\n",
    "        # Lower bounds\n",
    "        m.addConstr(zvar >= xL * yvar + yL * xvar - xL * yL, name=f\"{tag}_lb1\")\n",
    "        m.addConstr(zvar >= xU * yvar + yU * xvar - xU * yU, name=f\"{tag}_lb2\")\n",
    "        # Upper bounds\n",
    "        m.addConstr(zvar <= xU * yvar + yL * xvar - xU * yL, name=f\"{tag}_ub1\")\n",
    "        m.addConstr(zvar <= xL * yvar + yU * xvar - xL * yU, name=f\"{tag}_ub2\")\n",
    "\n",
    "    # === Constraints ===\n",
    "\n",
    "    # 1) Pre-activation split: sum_j w_ij x^n_j + b_i = p_i^n - q_i^n\n",
    "    for n in range(N):\n",
    "        for i in range(l1):\n",
    "            m.addConstr(\n",
    "                quicksum(w[i, j] * X[n, j] for j in range(d)) + b[i] == p[n, i] - q[n, i],\n",
    "                name=f\"preact[n{n},i{i}]\"\n",
    "            )\n",
    "\n",
    "    # 2) p,q gating with z (Big-M)\n",
    "    for n in range(N):\n",
    "        for i in range(l1):\n",
    "            m.addConstr(p[n, i] <= M_act * (1 - z[n, i]), name=f\"p_gate[n{n},i{i}]\")\n",
    "            m.addConstr(q[n, i] <= M_act * z[n, i],     name=f\"q_gate[n{n},i{i}]\")\n",
    "\n",
    "    # 3) Output: sum_i p_i^n * v_i = y^n  -> McCormick on phi_{n,i} = p_{n,i} * v_i\n",
    "    pL, pU = 0.0, M_act\n",
    "    vL, vU = -V_bound, V_bound\n",
    "    for n in range(N):\n",
    "        for i in range(l1):\n",
    "            add_mccormick(phi[n, i], p[n, i], v[i], pL, pU, vL, vU, tag=f\"phi[n{n},i{i}]\")\n",
    "        m.addConstr(quicksum(phi[n, i] for i in range(l1)) == y[n], name=f\"out[n{n}]\")\n",
    "\n",
    "    # 4) s_{ij} = w_{ij} * v_i  -> McCormick\n",
    "    wL, wU = -W_bound, W_bound\n",
    "    for i in range(l1):\n",
    "        for j in range(d):\n",
    "            add_mccormick(s[i, j], w[i, j], v[i], wL, wU, vL, vU, tag=f\"s[i{i},j{j}]\")\n",
    "\n",
    "    # 5) Link s_{ij} to sparsity selector t_{ij} via Big-M\n",
    "    for i in range(l1):\n",
    "        for j in range(d):\n",
    "            m.addConstr(-M_sparsity * t[i, j] <= s[i, j], name=f\"s_lb[i{i},j{j}]\")\n",
    "            m.addConstr( s[i, j] <=  M_sparsity * t[i, j], name=f\"s_ub[i{i},j{j}]\")\n",
    "\n",
    "    # 6) Objective: minimize sum_{i,j} t_{ij}\n",
    "    m.setObjective(quicksum(t[i, j] for i in range(l1) for j in range(d)), GRB.MINIMIZE)\n",
    "\n",
    "    # === Optimize ===\n",
    "    m.optimize()\n",
    "\n",
    "    if m.Status not in (GRB.OPTIMAL, GRB.TIME_LIMIT):\n",
    "        raise RuntimeError(f\"Gurobi ended with status {m.Status}\")\n",
    "\n",
    "    # Extract learned parameters\n",
    "    W = np.array([[w[i, j].X for j in range(d)] for i in range(l1)])\n",
    "    b_vec = np.array([b[i].X for i in range(l1)])\n",
    "    v_vec = np.array([v[i].X for i in range(l1)])\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nLearned parameters:\")\n",
    "        print(\"W =\\n\", W)\n",
    "        print(\"b =\", b_vec)\n",
    "        print(\"v =\", v_vec)\n",
    "\n",
    "    return {\"W\": W, \"b\": b_vec, \"v\": v_vec, \"model\": m}\n",
    "\n",
    "# -------------------------- Example usage --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Tiny toy data: N=6, d=3\n",
    "    rng = np.random.default_rng(0)\n",
    "    N, d, l1 = 6, 3, 4\n",
    "    X = rng.normal(size=(N, d))\n",
    "    # Create a synthetic y from a hidden ReLU-ish structure (just to have targets)\n",
    "    true_W = rng.uniform(-1, 1, size=(l1, d))\n",
    "    true_b = rng.uniform(-0.5, 0.5, size=(l1,))\n",
    "    true_v = rng.uniform(-1, 1, size=(l1,))\n",
    "    pre = X @ true_W.T + true_b  # (N, l1)\n",
    "    relu = np.maximum(pre, 0.0)\n",
    "    y = relu @ true_v\n",
    "\n",
    "    result = build_and_solve_nn_milp(X, y, l1,\n",
    "                                     W_bound=1.0, V_bound=1.0, B_bound=1.0,\n",
    "                                     time_limit=30, verbose=True, seed=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
