{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# XOR with a Tiny Neural Network (1 hidden layer, 2 neurons)\n", "\n", "This notebook builds and trains a minimal neural network **from scratch (NumPy only)** to solve the classic XOR problem.\n", "\n", "**Architecture**\n", "- Input: 2 features\n", "- Hidden layer: 2 neurons (tanh activation)\n", "- Output: 1 neuron (sigmoid)\n", "\n", "We'll train with binary cross-entropy and gradient descent, then plot the loss curve and decision boundary."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "np.set_printoptions(precision=4, suppress=True)\n", "\n", "# Reproducibility\n", "rng = np.random.default_rng(42)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# XOR dataset\n", "X = np.array([[0., 0.],\n", "              [0., 1.],\n", "              [1., 0.],\n", "              [1., 1.]], dtype=float)\n", "y = np.array([[0.], [1.], [1.], [0.]], dtype=float)\n", "m, d = X.shape\n", "print(f\"X shape: {X.shape}, y shape: {y.shape}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Model hyperparameters\n", "n_hidden = 2\n", "lr = 0.1\n", "epochs = 10000\n", "\n", "# Parameter initialization (small random weights)\n", "W1 = rng.normal(0.0, 0.5, size=(d, n_hidden))\n", "b1 = np.zeros((n_hidden,))\n", "W2 = rng.normal(0.0, 0.5, size=(n_hidden, 1))\n", "b2 = np.zeros((1,))\n", "\n", "def sigmoid(z):\n", "    return 1.0/(1.0 + np.exp(-z))\n", "\n", "def forward(X, W1, b1, W2, b2):\n", "    z1 = X @ W1 + b1  # (m, n_hidden)\n", "    a1 = np.tanh(z1)\n", "    z2 = a1 @ W2 + b2  # (m, 1)\n", "    yhat = sigmoid(z2)\n", "    cache = {\"z1\": z1, \"a1\": a1, \"z2\": z2, \"yhat\": yhat}\n", "    return yhat, cache\n", "\n", "def bce_loss(yhat, y):\n", "    eps = 1e-8\n", "    return -np.mean(y*np.log(yhat + eps) + (1-y)*np.log(1 - yhat + eps))\n", "\n", "loss_hist = []\n", "for t in range(epochs):\n", "    # Forward\n", "    yhat, cache = forward(X, W1, b1, W2, b2)\n", "    loss = bce_loss(yhat, y)\n", "    loss_hist.append(loss)\n", "\n", "    # Backward\n", "    # dLoss/dyhat = (yhat - y) / (yhat*(1 - yhat)) * dBCE/dlogit ?\n", "    # For sigmoid + BCE, gradient simplifies: dL/dz2 = (yhat - y)/m\n", "    dz2 = (yhat - y) / m                        # (m, 1)\n", "    dW2 = cache[\"a1\"].T @ dz2                   # (n_hidden, 1)\n", "    db2 = np.sum(dz2, axis=0)                   # (1,)\n", "\n", "    da1 = dz2 @ W2.T                             # (m, n_hidden)\n", "    dz1 = da1 * (1 - cache[\"a1\"]**2)            # tanh' = 1 - tanh^2\n", "    dW1 = X.T @ dz1                              # (d, n_hidden)\n", "    db1 = np.sum(dz1, axis=0)                   # (n_hidden,)\n", "\n", "    # Update\n", "    W2 -= lr * dW2\n", "    b2 -= lr * db2\n", "    W1 -= lr * dW1\n", "    b1 -= lr * db1\n", "\n", "    if (t+1) % 1000 == 0:\n", "        preds = (yhat >= 0.5).astype(float)\n", "        acc = np.mean(preds == y)\n", "        print(f\"epoch {t+1:5d} | loss {loss:.5f} | acc {acc:.2f}\")\n", "\n", "print(\"\\nFinal parameters:\")\n", "print(\"W1=\\n\", W1)\n", "print(\"b1=\\n\", b1)\n", "print(\"W2=\\n\", W2)\n", "print(\"b2=\\n\", b2)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Plot loss curve (one chart only)\n", "plt.figure()\n", "plt.plot(loss_hist)\n", "plt.title(\"Training Loss (Binary Cross-Entropy)\")\n", "plt.xlabel(\"Epoch\")\n", "plt.ylabel(\"Loss\")\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Plot decision boundary (one chart only)\n", "xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 300),\n", "                     np.linspace(-0.5, 1.5, 300))\n", "grid = np.c_[xx.ravel(), yy.ravel()]\n", "probs, _ = forward(grid, W1, b1, W2, b2)\n", "probs = probs.reshape(xx.shape)\n", "\n", "plt.figure()\n", "cs = plt.contourf(xx, yy, probs, levels=50, alpha=0.6)\n", "plt.contour(xx, yy, probs, levels=[0.5])\n", "X0 = X[y.ravel() == 0]\n", "X1 = X[y.ravel() == 1]\n", "plt.scatter(X0[:,0], X0[:,1], marker='o', label='class 0')\n", "plt.scatter(X1[:,0], X1[:,1], marker='s', label='class 1')\n", "plt.legend()\n", "plt.title(\"Decision Boundary after Training\")\n", "plt.xlabel(\"x1\")\n", "plt.ylabel(\"x2\")\n", "plt.xlim(-0.5, 1.5)\n", "plt.ylim(-0.5, 1.5)\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Final predictions on XOR inputs\n", "yhat, _ = forward(X, W1, b1, W2, b2)\n", "preds = (yhat >= 0.5).astype(int)\n", "print(\"Inputs:\\n\", X)\n", "print(\"Target:\\n\", y.astype(int))\n", "print(\"Predicted prob:\\n\", yhat)\n", "print(\"Predicted class:\\n\", preds)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 5}